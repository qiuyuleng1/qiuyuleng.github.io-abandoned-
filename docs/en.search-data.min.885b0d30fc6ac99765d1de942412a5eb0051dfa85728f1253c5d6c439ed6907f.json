[{"id":0,"href":"/stage2/docker-in-action/","title":"Docker in Action","section":"Stage2s","content":"Introduction Chapter 1: Welcome to Docker Docker make it easier for users to install and run the software. Docker is a tool helping solve common problems such as installing, removing, upgrading, distributing, trusting and running software, helping system administrators focus on higher-value activities. What is Docker? Docker is an open source project for building, shipping and running programs. containers Install Docker https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository Take care of the setting of proxy https://docs.docker.com/config/daemon/systemd/#httphttps-proxy https://docs.docker.com/network/proxy/ Hello World 本机上没有这个image 本机上有这个image Containers and Docker Containers can isolate a process from all resources except where explicitly allowed. However, it is challenging to to manually built correctly. Docker solved this problem. Docker makes container simpler to use. Containers are not virtualization Virtual machines require high resource overhead (because they run a whole OS) and requires a long time to startup. Docker (containers) solved this problem. Container is an OS feature. Running software in containers for isolation\nPrograms run inside a container can access only their own memory and resources as scoped by the container. Docker build container using 10 major system features to suit the needs of the contained software and fit the environment the container will run. Shipping containers image: component in the container (shipping units) Docker image is a bundled snapshot of all files needed for program running inside a container. Containers that were started from the same image don’t share changes to their filesystem. registries and indexes What problems does Docker solve? Get organized\nImprove portability 便携性 On macOS and Windows, Docker uses a single, small virtual machine to run all the containers. Developer could focus on writing programs on a single platform. Protect your computer also protect from the software inside a container\nWhy is Docker Important? \u0026amp; When and where to use Docker? Docker provides an abstraction.\nKeep your computer clean.\nSecurity issues.\nGetting help with the Docker command line docker help\ndocker help e.g. docker help ps\nPart 1: Process isolation and environment-independent computing Chapter 2: Running software in containers Controlling containers: Building a website monitor\nNGINX and mailer run as detached containers. Detached means the container will run in the background, without being attached to any input or output stream. Watcher runs as interactive container. Steps:\nCreating and starting a new container: detached containers docker run \u0026ndash;detach \u0026ndash;name web nginx:latest \u0026ndash;detach or -d means the program is started in the background and is not attached to your terminal (cannot be found in ps -l ), called daemon Running interactive containers docker run \u0026ndash;interactive \u0026ndash;tty \u0026ndash;link web:web \u0026ndash;name web_test busybox:1.29 /bin/sh \u0026ndash;interactive : keep the standard input open for the container even if no terminal is attached \u0026ndash;tty : allocate a virtual terminal for the container Usually use both of these when running an interactive program docker run -it \u0026ndash;name agent \u0026ndash;link web:insideweb \u0026ndash;link mailer:insidemailer dockerinaction/ch2_agent Hold ctrl and press P and then Q to return to the shell. Listing, stopping, restarting, and viewing output of containers docker ps shows information for each running containers docker logs container_name check the logs for container docker stop container_name stop a container Solved problems and the PID namespace Docker creates a new PID namespace for each container by default\ndocker exec container_name command run a command in a running container docker exec container_name ps to show all the running processes and their PID in the container docker run \u0026ndash;pid host busybox:1.29 ps to create containers without their own PID namespace. could see all process in the host server. Conflict problems could be solved by docker due to environment independence\nOther conflict problems that docker could solve:\nBy using Linux namespaces, resource limits, filesystem roots, virtualized network components. Eliminating metaconflicts: Building a website farm Flexible container identification: deal with container name collisions docker rename new-name old-name rename a container container ID, could use the first 12 chars. docker create creates a container in a stopped state. docker create \u0026ndash;cidfile /tmp/web.cid nginx write container ID (CID) to a known file. Docker won\u0026rsquo;t create a new container if the provided CID file already exits. To avoid conflict, we can use a partition path, such as /containers/web/customer1/web.cid docker ps \u0026ndash;latest \u0026ndash;quiet get the CID of the last container. docker could generates human-readable names for each container ( \u0026ndash;name flag override it) Container state and dependencies\ndocker ps -a to see all the containers The order of starting containers generate an IP addr when creating containers Building environment-agnostic systems To help build environment-agnostic system, Docker has three specific features:\nRead-only filesystems Environment variable injection Volumes Read-only filesystems docker inspect \u0026ndash;format \u0026ldquo;{{.State.Running}}\u0026rdquo; wp inspect the container metadata directly. \u0026ndash;format with Go template docker diff CONTAINER Inspect changes to files or directories on a container\u0026rsquo;s filesystem docker run -d \u0026ndash;name wp2 \u0026ndash;read-only -v /run/apache2/ \u0026ndash;tmpfs /tmp wordpress:5.0.0-php7.2-apache #!/bin/sh DB_CID=$(docker create -e MYSQL_ROOT_PASSWORD=ch2demo mysql:5.7) docker start $DB_CID MAILER_CID=$(docker create dockerinaction/ch2_mailer) docker start $MAILER_CID WP_CID=$(docker create \u0026ndash;link $DB_CID:mysql -p 80 \u0026ndash;read-only -v /run/apache2/ \u0026ndash;tmpfs /tmp wordpress:5.0.0-php7.2-apache) docker start $WP_CID AGENT_CID=$(docker create \u0026ndash;link $WP_CID:insideweb \u0026ndash;link $MAILER_CID:insidemailer dockerinaction/ch2_agent) docker start $AGENT_CID Environment variable injection Environment variable: change programs\u0026rsquo; configuration without modifying any files docker run \u0026ndash;env (-e) ENVIRONMENT_VAR=my_environment_vat busybox:1.29 env Injects an environment variable\nBuilding durable containers Restore the container service as quickly as possible.\nBasic strategy: automatically restarting a process when it exits or fails.\nAutomatically restarting containers \u0026ndash;restart flag\nBackoff strategy: determines the amount of time that should pass between successive restart attempts. Exponential backoff strategy Issues: containers waiting to be restarted are in the restarting state, and we cannot do anything that requires the container to be in a running state (such as docker exec ) Using PID 1 and init systems Init system: a program that\u0026rsquo;s used to launch and maintain the state of other programs. e.g.: runit, Yelp/dumb-init, tini, supervisord, and tianon/gosu. docker run -d -p 80:80 \u0026ndash;name lamp-test tutum/lamp docker top CONTAINER Display the running processes of a container An alternative method of init system: using a startup script docker run \u0026ndash;entrypoint=\u0026ldquo;cat\u0026rdquo; wordpress:5.0.0-php7.2-apache /usr/local/bin/docker-entrypoint.sh \u0026ndash;entrypoint Docker containers run something called an entrypoint before executing the command Cleaning up docker rm CONTAINER For container in running process, using docker stop first (better), or using docker rm -f CONTAINER (for quick stop). docker run \u0026ndash;rm Automatically remove the container as soon as it enters the exited state. docker rm -vf $(docker ps -a -q) Clean (kill) all the containers. Chapter 3: Software installation simplified\nIdentifying software What is a named repository? It is a named bucket of images, similar to a URL.\nA repository can hold several images, identified uniquely with tags.\nName and tag form a composite key, e.g., nginx:latest\nUsing tags A tag can only attach to one image, while one image can have several tags.\ncreate useful aliases different tags for different versioning different tags for different software configurations Finding and installing software\nFind software using index. Docker Hub is one of the public Docker indexes, and is the default registry and index used by docker.\nWorking with Docker registries from the command line docker login before publishing images.\nUsing alternative registries Registry address is part of the full repository specification.\n[REGISTRYHOST:PORT/][USERNAME/]NAME[:TAG]\nJust specify the registry host, e.g., docker pull quay.io/dockerinaction/ch3_hello_registry:latest\nWorking with images as files Using docker load command.\nBefore this, we need to save one image from a loaded image docker save -o filename.tar busybox:latest\nRemove image from docker docker rmi busybox\nLoad it again docker load -i myfile.tar\nCheck it docker images to list images.\nInstalling from a Dockerfile git clone https://github.com/dockerinaction/ch3_dockerfile.git\ndocker build -t dia_ch3/dockerfile:latest ch3_dockerfile\nUsing Docker Hub from the website https://hub.docker.com\nInstallation files and isolation layer: a set of files and file metadata that is packaged and distributed as an atomic unit, called intermediate images.\nImage layers in action Layer relationships Docker can uniquely identifies images and layers, it is able to recognize shared image dependencies between applications and avoid download those dependencies again.\nContainer filesystem abstraction and isolation: Union filesystem Union filesystem (UFS) is used to create mount point and abstract the use of layers.\nWhen Docker creates a container, that new container will have its own MNT namespace, and a new mount point will be created for the container to the image. chroot is used to prevent from referring any other part of the host filesystem in the container. Benefits of this toolset and filesystem structure Common layers need to be installed only once. Layers could manage dependencies and separating concerns. Create software specializations when you can layer minor changes on top of a basic image. Weaknesses of union filesystems Often need to translate between the rules of different filesystems. Chapter 4: Working with storage and volumes Manage data with containers: where is the data stored? what happens to that data when you stop the container or remove it?\nFile trees and mount points the image that a container is created from is mounted at that container’s file tree root, or the / point every container has a different set of mount points Containers get access to storage on the host filesystem and share storage between containers: mount nonimage-related storage at other points in a container file tree. Three types of storage mounted into containers: bind mount in-memory storage docker volumes\n\u0026ndash;mount flag in docker run and docker create Bind mounts Bind mounts attach a user-specified location on the host filesystem to a specific point in a container file tree.\nhost provides files or dictionaries need by the program running in a container. containerized program produces a file or log need to be processed outside containers. An example\ntouch ~/example.log cat \u0026gt;~/example.conf \u0026laquo;EOF server { listen 80; server_name localhost; access_log /var/log/nginx/custom.host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; } } EOF CONF_SRC=~/example.conf; CONF_DST=/etc/nginx/conf.d/default.conf; LOG_SRC=~/example.log; LOG_DST=/var/log/nginx/custom.host.access.log; docker run -d \u0026ndash;name diaweb \\\n\u0026ndash;mount type=bind,src=${CONF_SRC},dst=${CONF_DST},readonly=true \u0026ndash;mount type=bind,src=${LOG_SRC},dst=${LOG_DST} -p 80:80 nginx:latest src : source location on the host file tree (name of the volume, for anonymous volumes, this field is omitted) dst : destination location on the container file tree readonly=true prevent modifying the content of the volume Test: docker exec diaweb sed -i \u0026ldquo;s/listen 80/listen 8080/\u0026rdquo; /etc/nginx/conf.d/default.conf Problems:\nportable conflict with other containers In-memory storage Private files should use in-memory storage, instead of including in an image or writing them to disk.\ndocker run \u0026ndash;rm \u0026ndash;mount type=tmpfs,dst=/tmp,tmpfs-size=16k,tmpfs-mode=1770 \u0026ndash;entrypoint mount alpine:latest -v Set the type option on the mount flag to tmpfs. File permissions 1770 (1777 for default) Size limit 16k Docker volumes Introduction to volumes It is used to organizing data: in disk space that can be accessed by containers.\ndo not need to concern the system (fill on any machine with Docker installed). easy to clean up for Docker (life cycle) It could decouple storage from specialized locations on the filesystem that you might specify with bind mounts, and could run o any machine without considering conflict.\nUsing docker volume subcommand set.\ndocker volume create \u0026ndash;driver local \u0026ndash;label example=location location-example\ndocker volume inspect \u0026ndash;format \u0026ldquo;{{json .Mountpoint}}\u0026rdquo; location-example\n–-driver local: create a directory to store the contents of volume in a part of the host filesystem under control of the Docker engine.\nHow to share volumes between containers without exposing the exact location of managed containers?\nVolumes provide container-independent data management images are appropriate for packaging and distributing relatively static files such as programs (reusable) volumes hold dynamic data or specializations (simple to share). Polymorphic 多态：using volumes, do not need to modify an image.\nUsing volumes with a NoSQL database Create a single-node Cassandra cluster, create a keyspace, delete the container, and then recover that keyspace on a new node in another container:\ndocker volume create \u0026ndash;driver local \u0026ndash;label example=cassandra cass-shared\ndocker run -d \u0026ndash;volume cass-shared:/var/lib/cassandra/data \u0026ndash;name cass1 cassandra:2.2 Shared mount points and sharing files Sharing access to the same set of files between multiple containers.\nBind mounts methods: may lead to conflict. Use volumes: do not need the knowledge of the underlying host filesystem. Anonymous volumes and the volumes-from flag Volume is created without a human-friendly name, and is assigned a unique identifier to eliminate volume-naming conflicts.\n\u0026ndash;volumes-from : copy the definition from other containers to the new container. (copy the full volume definition, including mount point, permission)\nSituations not suit for \u0026ndash;volumes-from : when you need tomount to a different location conflict: volumes from different containers with the same mount point\ndocker run \u0026ndash;name chomsky \u0026ndash;volume /library/ss alpine:latest echo \u0026ldquo;Chomsky collection created.\u0026rdquo; docker run \u0026ndash;name lamport \u0026ndash;volume /library/ss alpine:latest echo \u0026ldquo;Lamport collection created.\u0026rdquo;\ndocker run \u0026ndash;name student \u0026ndash;volumes-from chomsky \u0026ndash;volumes-from lamport alpine:latest ls -l /library/\ndocker inspect -f \u0026ldquo;{{json .Mounts}}\u0026rdquo; student\nOnly one of the two containers is mounted. # when you need to change the write permission of a volume Cleaning up volumes docker volume list to list all the volumes\ndocker volume remove VOLUME_NAME/Unique_identifier to remove the volume\nsupport list argument, such as docker volume remove amazon google microsoft volume in use cannot be remove docker volume prune delete all volumes that can be deleted\n\u0026ndash;filter : filter –-force : without confirmation step Advanced storage with volume plugins For a cluster of machines, need to choose storage infrastructure, such as network storage\nneed to select proper Docker plugin.\nChapter 5: Single-host networking How to connect a container to the network.\nNetworking background Basics: Protocols, interfaces, and ports Protocol: communication and networking in a sort of language, such as HTTP Interface: address, represents a location IP address: unique, information about their location on their network Ethernet interface and loopback interface Port: a recipient or a sender Bigger picture: Networks, NAT and port forwarding bridge: an interface that connects multiple networks so that they can function as a single network. selectively forwarding traffic between the connected networks based on another type of network address. Docker container networking Benefits: 1. keep environment agnosticism 2. adopt specific network implementation\nProblems: hard to determine the IP address of host, and inhibits container to other services outside the container network.\ndocker network subcommands\ndocker network ls to list all network of Docker.\nbridge: inter-container connectivity for all containers running on the same machine host: not create any networking namespace for attached container. Container will interact with host\u0026rsquo;s network like uncontained processes. none: no network connectivity scope: local, global and swarm local: the network is constrained to the machine where the network exists global: created on every node in a cluster but not route between them swarm: span all of the hosts participating in a Docker swarm Creating a user-defined bridge network Local to the machine where Docker is installed, creates route between participating containers and the wider network where the host is attached.\nshape bridge network to fit your environment\ndocker network create \u0026ndash;driver bridge \u0026ndash;label project=dockerinaction \u0026ndash;label chapter=5 \u0026ndash;attachable \u0026ndash;scope local \u0026ndash;subnet 10.0.42.0/24 \u0026ndash;ip-range 10.0.42.128/25 user-network\ndocker network create \u0026ndash;driver bridge \u0026ndash;label project=dockerinaction \u0026ndash;label chapter=5 \u0026ndash;attachable \u0026ndash;scope local \u0026ndash;subnet 10.0.43.0/24 \u0026ndash;ip-range 10.0.43.128/25 user-network2 attach a running container to more than one network\ndocker run -it \u0026ndash;network user-network \u0026ndash;name network-explorer alpine:3.8 sh\nip -f inet -4 -o addr\ndocker network connect user-network2 network-explorer\nip -f inet -4 -o addr nmap -sn 10.0.42.* -sn 10.0.43.* -oG /dev/stdout | grep Status what those networks look like to running software inside an attached container\ndocker run -d \u0026ndash;name lighthouse \u0026ndash;network user-network2 alpine:3.8 sleep 1d\nnmap -sn 10.0.42.* -sn 10.0.43.* -oG /dev/stdout | grep Statu For situations that route traffic between containers on different machines, can use underlay networks provided by the macvlan or ipvlan network drivers.\nor use swarm mode Special container networks: host and none software inside the container have the same degree of access to the host network as software outside container\ndocker run \u0026ndash;rm \u0026ndash;network host alpine:3.8 ip -o addr software in the container cannot reach anything outside the container (network isolate)\ndocker run \u0026ndash;rm \u0026ndash;network none alpine:3.8 ip -o addr\ndocker run \u0026ndash;rm \u0026ndash;network none alpine:3.8 ping -w 2 1.1.1.1 Handling inbound traffic with NodePort publishing Need to tell Docker how to forward traffic from the external network interfaces: specify TCP or UDP port.\nNodePort publishing: match Docker and other ecosystem projects.\nProvided at container creation time and cannot be changed later docker run/create \u0026ndash;publish colon-delimited string argument.\nExamples:\ndocker run \u0026ndash;rm -p 8080 alpine:3.8 echo \u0026ldquo;forward ephemeral TCP -\u0026gt; container TCP 8080\u0026rdquo; Host operating system will select a random host port, and traffic will be routed to port 8080 in the container. Since ports are scarce resources, and this way will avoid potential conflicts. Programs running inside a container do not know which port is being forwarded from the host use docker port listener to look up port mappings lookup special container port and protocol\ndocker run -d -p 8080 -p 3000 -p 7500 \u0026ndash;name multi-listener alpine:3.8 sleep 300\ndocker port multi-listener 3000 Container networking caveats and customizations No firewalls or network policies Docker bridge networks do not provide any network firewall or access-control functionality.\nCustom DNS configuration at /etc/hosts inside your container\ndocker run \u0026ndash;hostname set hostname of a new container, other containers do not know this hostname. docker run \u0026ndash; dns use an external DNS server docker run \u0026ndash;dns-search=[] provide a default hostname suffix. e.g., docker run \u0026ndash;rm \u0026ndash;dns-search docker.com alpine:3.8 nslookup hub will resolve the IP addr of hob.docker.com docker run \u0026ndash;add-host Externalizing network management use Docker none network + third party container-aware tools.\nChapter6: Limiting risk with resource controls Set resource allowances By default, Docker containers may use unlimited CPU, memory, and device I/O resources.\nMemory limits docker run/create \u0026ndash;memory where unit = b, k, m or g Note that they are not reservations, and only a protection from overconsumption. Is the memory enough for the container? docker stats CONTAINER_NAME Some programs may fail with a memory access fault, whereas others may start writing out-of-memory errors to their logging. \u0026ndash;restart CPU specify the relative weight of a container to other containers \u0026ndash;cpu-shares flag. Only enforced only when there is contention for time on the CPU limit the total amount of CPU used by a container \u0026ndash;cpus the number of CPU cores the container should be able to use enforced assign a container to a specific CPU set. avoid context switch Access to devices More like resource-authorization control than a limit.\n\u0026ndash;device : + a map between the device file on the host operating system and the location inside the new container\nSharing memory Docker creates a unique IPC namespace for each container by default, which prevents processes in one container from accessing the memory on the host or in other containers\nSharing IPC primitives between containers \u0026ndash;ipc flag\nrun programs that communicate with shared memory in different containers create a new container in the same IPC namespace as another target container\nsharing memory with host \u0026ndash;ipc=host Understanding users User is specified by the image metadata by default, often the root user.\nWorking with the run-as user docker inspect \u0026ndash;format \u0026ldquo;{{.Config.User}}\u0026rdquo; busybox:1.29 : if blank, the container will default to running as the root user.\nuser might be changed docker container run \u0026ndash;rm \u0026ndash;entrypoint \u0026quot;\u0026quot; busybox:1.29 whoami\ndocker container run \u0026ndash;rm \u0026ndash;entrypoint \u0026quot;\u0026quot; busybox:1.29 id\nTo run as specific user when creating the container, the username must exist on the image you are using.\nget a list of available user in an image with docker container run \u0026ndash;rm busybox:1.29 awk -F: \u0026lsquo;$0=$1\u0026rsquo; /etc/passwd docker container run \u0026ndash;rm \u0026ndash;user nobody busybox:1.29 id Users and volumes User ID space is shared: both root on the host and root in the container have user ID 0.\ncontainer\u0026rsquo;s root can access a file owned by root on the host do not mount the file into that container with a volume echo \u0026ldquo;e=mc^2\u0026rdquo; \u0026gt; garbage chmod 600 garbage sudo chown root garbage docker container run \u0026ndash;rm -v \u0026ldquo;$(pwd)\u0026quot;/garbage:/test/garbage -u nobody ubuntu:16.04 cat /test/garbage docker container run \u0026ndash;rm -v \u0026ldquo;$(pwd)\u0026quot;/garbage:/test/garbage -u root ubuntu:16.04 cat /test/garbage\nOutputs: \u0026ldquo;e=mc^2\u0026rdquo; # Specify the desired user and group ID to avoid file permission conflicts.\nmkdir logFiles sudo chown 2000:2000 logFiles docker container run \u0026ndash;rm -v \u0026ldquo;$(pwd)\u0026quot;/logFiles:/logFiles -u 2000:2000 ubuntu:16.04 /bin/bash -c \u0026ldquo;echo This is important info \u0026gt; /logFiles/important.log\u0026rdquo; docker container run \u0026ndash;rm -v \u0026ldquo;$(pwd)\u0026quot;/logFiles:/logFiles -u 2000:2000 ubuntu:16.04 /bin/bash -c \u0026ldquo;echo More info \u0026raquo; /logFiles/important.log\u0026rdquo; Be careful about which user can control the Docker daemon.\nIntroduction to the Linux user namespace and UID remapping By default, Docker containers do not use the USR namespace\nA container running with a user ID that is the same as a user on the host machine has the same host file permissions as that user. When user namespace is enable: UID namespace remapping\nremap to unprivileged UID dockremap user Adjusting OS feature access with capabilities Capabilities: OS feature authorization\nList all default caoabilities:\ndocker container run \u0026ndash;rm -u nobody ubuntu:16.04 /bin/bash -c \u0026ldquo;capsh \u0026ndash;print | grep net_raw\u0026rdquo;\n\u0026ndash;cap-drop : drop capabilities during container run/create\n\u0026ndash;cap-add : add capabilities\nRunning a container with full privileges Run a system administration task: privileged containers\nPrivileged containers maintain their filesystem and network isolation but have full access to shared memory and devices and possess full system capabilities.\n\u0026ndash;privileged for docker container create/run\nStrengthening containers with enhanced tools Specifying additional security options \u0026ndash;security-opt\nSELinux: labeling\nAppArmor: file path\nBuilding use-case-appropriate containers Start with the most isolated container you can build and justify reasons for weakening those restrictions.\nOr use default container construction.\nApplications running as a user with limited permissions limit the system capabilities set limits on how much of the CPU and memory the application can use specifically whitelist devices that it can access High-level system services Often require privileged access, including cron, syslogd, sshd and so on.\nUse capabilities to tune their access.\nLow-level system services Require privileged access, including devices and system\u0026rsquo;s network stack. Rare to run inside containers.\nUsed for short-running configuration containers: change the configuration with a privileged containers.\nPart 2: Packaging software for distribution Chapter 7: Packaging software in images Building Docker images from a container\nTips: docker run is the same as docker container run. https://stackoverflow.com/questions/51247609/difference-between-docker-run-and-docker-container-run\ndocker container diff CONTAINER_NAME review the list of files that have been modified in a container (added A, changed C, deleted D) docker container commit commit changes to new image -a flag: author -m flag: commit message add \u0026ndash;entrypoint Parameters that carry forward with an image created from the container.\nall environment variables working directory the set of exposed ports all volume definitions the container entrypoint command and arguments Note: If not specifically set for the container, the values will be inherited from the original image Going deep on Docker images and layers Exploring union filesystems Each time a change is made to a union filesystem, that change is recorded on a new layer on top of all of the others\nWhen reading a file, the file will be read from the topmost layer where it exist.\nIf a file was not created or changed on the top layer, the read will fall through the layers until it reaches a layer where that file does exist When a file is deleted, a delete record is written to the top layer, which hides any versions of that file on lower layers.\nWhen a file is changed, that change is written to the top layer, which again hides any versions of that file on lower layers\nReintroducing images, layers, repositories, and tags Metadata of layer: generated identifier, identifier of the layer below it, execution context.\nImage: stack of layers, constructed by starting with a given top layer and then following all the links\ndocker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] commit container with name and tag.\ndocker tag copy image by creating a new tag or repo from existing one.\nAll layers below the writable layer created for a container are immutable.\nthey can never be modified make individual layers reusable share access to images instead of creating independent copies for every container any time making changes to an image, you need to add a new layer, and old layers are never removed. Managing image size and layer limits Union filesystem makes a file as deleted by actually adding a file to the top layer.\ndocker image history IMAGE show the history of an image.\nExporting and importing flat filesystems image \u0026amp; TAR file: bad idea, will lose the original image\u0026rsquo;s metadata and change history.\ndocker image save save the image to a TAR file. docker image import import the TAR file into docker. docker import -c \u0026ldquo;ENTRYPOINT [\u0026quot;/hello\u0026quot;]\u0026rdquo; - dockerinaction/ch7_static \u0026lt; static_hello.tar -c specify a Dockerfile command\nat the end of the first line: the content of the tarball will be streamed through stdin. This position can be file/URL/- small size, only one layer (called flat image), no layer reuse use creating branch docker container export Export a container\u0026rsquo;s filesystem as a tar archive docker cp Copy files/folders between a container and the local filesystem\nVersioning best practices\nlatest: default tag, should point to the latest stable build of its software instead of the true latest.\nEach row represents a distinct image.\nChapter 8: Building images automatically with Dockerfiles Dockerfile: text file containing instructions for building an image.\nPackaging Git with Dockerfile\nAn example Dockerfile for installing Git on Ubuntu # FROM ubuntu:latest LABEL maintainer=\u0026ldquo;dia@allingeek.com\u0026rdquo; RUN apt-get update \u0026amp;\u0026amp; apt-get install -y git ENTRYPOINT [\u0026ldquo;git\u0026rdquo;] docker image build \u0026ndash;tag ubuntu-git:auto .\n#: comments The first instruction of Dockerfile must be FROM . : the location of the Dockerfile is the current directory. \u0026ndash;file specify the name of Dockerfile, it usually be xxxx.df –quite / -q suppress the output of the build process The use of caching: the builder can cache the results of each step. If a problem occurs after several steps, the builder can restart from the same position after the problem has been fixed.\n\u0026ndash;no-cache : disable caching A Dockerfile primer Metadata instructions Benefits of using Dockerfile: simplify copying files from host computer into an image\ndefine which files should never be copied into any images: .dockerignore Each Dockerfile instruction will result in a new layer being created, so merge instructions to minimize the size of images and layer count when possible.\nFROM debian:buster-20190910 LABEL maintainer=\u0026ldquo;dia@allingeek.com\u0026rdquo; RUN groupadd -r -g 2200 example \u0026amp;\u0026amp; useradd -rM -g example -u 2200 example ENV APPROOT=\u0026quot;/app\u0026rdquo; APP=\u0026ldquo;mailer.sh\u0026rdquo; VERSION=\u0026ldquo;0.6\u0026rdquo; LABEL base.name=\u0026ldquo;Mailer Archetype\u0026rdquo; base.version=\u0026quot;${VERSION}\u0026rdquo; WORKDIR $APPROOT ADD . $APPROOT ENTRYPOINT [\u0026quot;/app/mailer.sh\u0026rdquo;] EXPOSE 33333\nDo not set the default user in the base otherwise # implementations will not be able to update the image # USER example:example # Some Dockerfile instructions:\nFROM : set the layer stack to start from which image. ENV : set environment variables for an image, similar to \u0026ndash;env flag on docker container run/create can also be used in other instructions in this Dockerfile LABEL : defile key:value pairs, similar to \u0026ndash;label on docker run/create\nadd additional metadata, will be available to process running inside a container WORKDIR : set default working directory EXPOSE : open TCP port ENTRYPOINT : set the executable to run at container startup has two forms: shell form and exec form if the shell form is used, all other arguments provided by CMD instruction or extra arguments will be ignored. app/mailer.sh does not exist USER set user and group for further build steps set up user and group accounts in base image let the implementations set the default user use docker inspect to inspect the environment variables of the image. ENV, LABEL, WORKDIR, VOLUME, EXPOSE, ADD, COPY Filesystem Instructions Some instructions that modify the filesystem\nFROM dockerinaction/mailer-base:0.6 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y netcat COPY [\u0026rdquo;./log-impl\u0026quot;, \u0026ldquo;${APPROOT}\u0026rdquo;] RUN chmod a+x ${APPROOT}/${APP} \u0026amp;\u0026amp; chown example:example /var/log USER example:example VOLUME [\u0026quot;/var/log\u0026quot;] CMD [\u0026quot;/var/log/mailer.log\u0026quot;] docker image build -t dockerinaction/mailer-logging -f mailer-logging.df .\nCOPY : at least two argus, source files + destination file ownership set to root support shell style and exec style arguments (exec is better) ADD : fetch remote source files if a URL is specified (not good, did not provide mechanism for cleaning up unused files and results in additional layers) extract the files of any resource determined to be an archive file (auto-extraction) VOLUME : similar to \u0026ndash;volume on docker run/create CMD : related to ENTRYPOINT\nrepresents an argument list for entrypoint if no ENTRYPOINT is set, ignore this instruction If ENTRYPOINT is set and using exec form, you use CMD to set default arguments. Tip: what is shell style and exec style?\nshell style: looks like a shell command with whitespace-delimited arguments can use \u0026ldquo;\u0026quot; to continue a single instruction onto the next line e.g.:\nRUN \u0026lsquo;source $HOME/.bashrc;\\ echo $HOME ' exec form: a string array in which the first value is the command to execute, the remaining values are argus. e.g.: RUN [\u0026quot;/bin/bash\u0026rdquo;, \u0026ldquo;-c\u0026rdquo;, \u0026ldquo;echo hello\u0026rdquo;] Injecting downstream build-time behavior ONBUILD : defines other instructions to execute if the resulting image is used as a base for another build.\nrecorded in the resulting image\u0026rsquo;s metadata under ContainerConfig.OnBuild in the downstream Dockerfiles, those ONBUILD instructions are executed after FROM instruction and before the next instruction. Creating maintainable Dockerfiles ARG : define variable that users can provide to Docker when building an image.\n\u0026ndash;build-arg = Multistage Dockerfile: a Dockerfile that has multiple FROM instructions\neach FROM instruction makes a new build stage FROM \u0026hellip; AS to name build stage, this name can be used in subsequent FROM and COPY \u0026ndash;from=\u0026lt;name|index\u0026gt; example:\n#################################################\nDefine a Builder stage and build app inside it # FROM golang:1-alpine as builder\nInstall CA Certificates # RUN apk update \u0026amp;\u0026amp; apk add ca-certificates\nCopy source into Builder # ENV HTTP_CLIENT_SRC=$GOPATH/src/dia/http-client/ COPY . $HTTP_CLIENT_SRC WORKDIR $HTTP_CLIENT_SRC\nBuild HTTP Client # RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -v -o /go/bin/http-client #################################################\nDefine a stage to build a runtime image. # FROM scratch as runtime ENV PATH=\u0026quot;/bin\u0026quot;\nCopy CA certificates and application binary from builder stage # COPY \u0026ndash;from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ca-certificates.crt COPY \u0026ndash;from=builder /go/bin/http-client /http-client ENTRYPOINT [\u0026quot;/http-client\u0026quot;] Using startup scripts and multiprocess containers Environmental preconditions validation failing fast and precondition validation are best practices in software design.\nImage author can introduce environment and dependency validation prior to execution of the main task, and to better inform user if the container is failed.\ne.g., WordPress use a script as the container entrypoint to validate environment variables and container links. The startup process could validate\nPresumed links (and aliases) Environment variables Secrets Network access Network port availability Root filesystem mount parameters (read-write or read-only) Volumes Current user Can use shell scripts to finish the above validations.\nExample: validate either another container has been linked to the web alias and has exposed port 80, or the WEB_HOST environment variable has been defined:\n#!/bin/bash set -e if [ -n \u0026ldquo;$WEB_PORT_80_TCP\u0026rdquo; ]; then if [ -z \u0026ldquo;$WEB_HOST\u0026rdquo; ]; then WEB_HOST=\u0026lsquo;web\u0026rsquo; else echo \u0026gt;\u0026amp;2 \u0026lsquo;[WARN]: Linked container, \u0026ldquo;web\u0026rdquo; overridden by $WEB_HOST.\u0026rsquo; echo \u0026gt;\u0026amp;2 \u0026ldquo;===\u0026gt; Connecting to WEB_HOST ($WEB_HOST)\u0026rdquo; fi fi if [ -z \u0026ldquo;$WEB_HOST\u0026rdquo; ]; then echo \u0026gt;\u0026amp;2 \u0026lsquo;[ERROR]: specify container to link; \u0026ldquo;web\u0026rdquo; or WEB_HOST env var\u0026rsquo; exit 1 fi exec \u0026ldquo;$@\u0026rdquo; # run the default command The same containers may fail later for other reasons → can combine startup scripts with container restart policies.\nInitialization processes Initialization (init) process in UNIX-based computers to start all the other system services, keep them running, and shut them down.\nUse light-weight init-style system for container as entrypoint, such as runit, tini, BusyBox init, Supervisord, and DAEMON tools.\n\u0026ndash;init: tini program by default evaluate using which init program. The purpose and use of health checks Health checks: check whether the application running inside the container is able to perform its function.\nhealth check command:\nHEALTHCHECK instruction in Dockerfile: should be reliable, lightweight, and not interfere with the operation of the main application because it will be executed frequently (every 30 seconds by default) 0: success, 1: unhealthy, 2: reserved three failed checks from healthy to unhealthy || exit 1\nTime-out : a time-out for health check command to run and exit Start period : grace period docker run \u0026ndash;health-cmd override the health check defined in the image if exists. Building hardened application images Hardening an image means reducing the attack surface inside any Docker containers based on it.\nStrategy:\nminimize the software included with the container enforce that your images are built from a specific image have a sensible default user eliminate a common path for root user escalation from programs with SUID or SGID attributes set Content-addressable image identifiers Content-addressable image identifier (CAIID) refers to a specific layer containing specific content, to enforce a build from a specific and unchanging starting point.\nAppend an symbol @ followed by the digest in place of the standard tag position\nUseful when incorporating known updates to a base into your images and identifying the exact build of the software running on your computer.\nPrevent from changing without your knowledge.\nUser permissions Docker cannot prevent from user run with root permission.\nUSER instruction in Dockerfile to set user and group, similar to docker container run/create .\nThe key is to determine the earliest appropriate time to drop privileges.\ntoo early: active user may not have permission to complete the instructions in Dockerfile Also need to consider the permissions and capabilities needed at runtime, such as system port range (1-1024) Which user should be dropped into?\nimage user do not know about Docker daemon configuration about namespace remap.\ngosu SUID and SGID permissions SUID: file will always execute as its owner, such as /usr/bin/passwd.\nHave the risk of compromise the root account inside a container.\nThe files with SUID and SGID are rarely required for application use cases.\nUnset the SUID and SGID permission for all files in current image:\nRUN for i in $(find / -type f ( -perm /u=s -o -perm /g=s )); do chmod ug-s $i; done\nChapter 9: Public and private software distribution Choosing a distribution method How to choose the appropriate method for your situation.\nA distribution spectrum\nSelection criteria Cost Visibility: secret projects or public works Transportation: transportation speed and bandwidth overhead Longevity Availability \u0026hellip; Publishing with hosted registries Hosted registry: a Docker registry service that is owned and operated by a third-party vendor.\ne.g., Docker Hub, Quay.io, Google Container Registry by default, Docker publishes to Docker Hub Publishing with public repositories: \u0026ldquo;Hello World!\u0026rdquo; via Docker Hub docker login Maintains a map of your credentials for registries that you authenticate.\ndocker image push insert Docker Hub username\u0026gt;/hello-dockerfile Push your repo to the host registry.\ndocker search username Searching repo owned by specific user.\nPrivate hosted repositories Before you can use docker image pull or docker container run to install an image from a private repository, you need to authenticate with the registry where the repository is hosted\nIntroducing private registries Using Docker\u0026rsquo;s registry software\nUsing the registry image Start a local registry in a container:\ndocker run -d -p 5000:5000 -v \u0026ldquo;$(pwd)\u0026quot;/data:/tmp/registry-dev \u0026ndash;restart=always \u0026ndash;name local-registry registry:2 docker image tag dockerinaction/ch9_registry_bound localhost:5000/dockerinaction/ch9_registry_bound docker image push localhost:5000/dockerinaction/ch9_registry_bound Where can we find the image localhost:5000/dockerinaction/ch9_registry_bound?\nRegistry stores the image on /var/lib/registry on its union filesystem, which is volume on the host filesystem by default. So we can find it in the volume of local-registry container. Use docker volume ls to list all the volumes. Use docker volume inspect xxx to display detailed information on the volume.\nNote that we can use -v xxx:/var/lib/registry to mount the /var/lib/registry on a specific location.\nFor \u0026ldquo;$(pwd)\u0026quot;/data:/tmp/registry-dev , mount the volume into the container. The first part \u0026ldquo;$(pwd)\u0026quot;/data is the location of the volume on host machine, the second part is the location on the container union filesystem. Note that in this example, we did not mount /var/lib/registry on this location, so we cannot find the image push to the registry on \u0026ldquo;$(pwd)\u0026quot;/data .\nWhen you’ve started the registry, you can use it like any other registry with docker pull, run, tag, push commands.\nConduct tight version control: pull images from external sources such as Docker Hub and copy them into their own registry. To ensure that an important image does not change or disappear unexpectedly when the author updates or removes the source image.\nConsuming images from your registry docker image rm dockerinaction/ch9_registry_bound localhost:5000/dockerinaction/ch9_registry_bound\ndocker image pull localhost:5000/dockerinaction/ch9_registry_bound This will prevent remote Docker clients from using your registry.\nManual image publishing and distribution Work with images as files.\ndocker image save + docker container export\ndocker image load + docker image import\nImage source-distribution workflows Using github.\nMuch flexible.\nChapter 10: Image pipelines Image build pipeline: preparing image material, building an image, testing, publishing images to registries.\nGoals of an image build pipeline\nApplication artifacts: runtime scripts and configuration files produced by software authors.\nCI tools.\nPatterns for building images How many images an application includes?\nAll-in-One Build plus Runtime Build plus Multiple Runtimes\nAll-in-one images Build the application into an image.\nDownsides:\ncontain more tools than necessary, easy for attackers\nthe size will be large (708MB)\nENTRYPOINT [\u0026ldquo;java\u0026rdquo;,\u0026quot;-jar\u0026rdquo;,\u0026quot;/app.jar\u0026rdquo;]\nSeparate build and runtime images The build-specific tools such as Maven and intermediate artifacts are no longer included in the runtime image.\nMuch smaller size (from 708MB to 401MB) Smaller attack surface. Variations of runtime image via multi-stage builds Multi-stage builds can be used to keep the specialized image synchronized with the application image and avoid duplication of image definitions.\nName a build stage:\neasily to mention the stage by other stages. build processes can build a stage by specifying that name as a build target. docker image build \u0026ndash;target : select the stage to build the image.\nIf do not specify a build stage, docker will build image from the last stage defined in the Dockerfile.\nAdd a trivial build stage at the end of the Dockerfile.\nEnsure app-image is the default image built with this Dockerfile # FROM app-image as default Record metadata at image build time Use LABEL instruction to capture metadata, at least including:\napplication name application version build date and time version-control commit identifier Commonly used labels http://label-schema.org/rc1/: label schema, e.g., LABEL org.label-schema.version=\u0026quot;${BUILD_ID}\u0026rdquo;\nOrchestrating the build with make Make: used to build programs, understands dependencies between the steps of a build process.\nSteps: gathering metadata, building application, building, testing, tagging the image.\nUse linting tool named hadolint to check Dockerfile.\nTesting images in a build pipeline Container Structure Test tool (CST) https://github.com/GoogleContainerTools/container-structure-test verifies the construction of a Docker image.\nverify desired file permissions and ownership, commands execute with expected output, and the image contains particular metadata such as a label or command the tool operates on arbitrary images without requiring any tooling or libraries to be included inside the image. Patterns for tagging images Important image-tagging features:\nTags are human-readable Multiple tags may point to a single image ID Tags are mutable and may be moved between images. Background Image tag mutation is commonly used to identify the latest image in a series.\nHowever, it is difficult to identify what does the latest lag mean?\nContinuous delivery with unique tags Using unique BUILD_ID tag.\nInconvenient for users.\nConfiguration image per deployment stage A generic, environment-agnostic application image. A set of environment-specific configuration image. Semantic versioning A version number of the form Major.Minor.Patch. Author could increment the following:\nMajor version when making incompatible API changes Minor version when adding functionality in a backward-compatible manner Patch version when making backward-compatible bug fixes make tag BUILD_ID=$BUILD_ID TAG=1.0.0\nHigher-level abstractions and orchestration Chapter 11: Service with Docker and Compose Service: any processes, functionality, or data that must be discoverable and available over a network is called a service.\nA service \u0026ldquo;Hello World!\u0026rdquo; Docker services are available only when Docker is running in swarm mode.\ndocker swarm init\ndocker service create \u0026ndash;publish 8080:80 \u0026ndash;name hello-world dockerinaction/ch11_service_hw:v1\nTask is a swarm concept that represents a unit of work. Each task has one associated container.\nAutomated resurrection and replication docker service ls to list the running services.\ndocker service ps hello-world to list the containers associated with a specific service.\nThe desired state is what the user wants the system to be doing, or what it is supposed to be doing.\nThe current state describes what the system is actually doing.\nOrchestrators remember how a system should be operating and manipulate it without being asked to do so by a user.\nyou need to understand how to describe systems and their operation. docker service inspect hello-world to output the current desired state definition for the service:\nName of the service Service ID Versioning and timestamps A template for the container workloads A replication mode: replicated mode and global mode. replicated mode (default) docker service scale hello-world=3 (use docker container ps and docker service ps hello-world to show the effects.) If you scale the service back down (docker service scale hello-world=2), you’ll also notice that higher-numbered containers are removed first. global mode Run one replica on each node in the swarm cluster. Rollout parameters Similar rollback parameters A description of the service endpoint Automated rollout\nIllustration of a Docker swarm\u0026rsquo;s action when deploying an update:\nService converged: the current state of the service is the same as the desired state described by the command.\nService health and rollback docker service update \u0026ndash;rollback hello-world 回滚\n\u0026ndash;update-failure-action to tell Swarm that failed deployment should roll back.\nupdate-max-failure-ratio to tell Swarm to tolerate start failures as long as 0.6 of the fleet is good. After that, the whole deployment will be marked as failed and a rollback will be initiated.\nAdd health check:\nAt last, remove the service: docker service rm hello-world\nDeclarative service environments with Compose V3 Imperative tools: such as docker service create Declarative tools: describe the new state of a system, rather than the steps required to change from the current state to the new state. When the service is complex, the number of imperative commands required to achieve the goals is too large. Then we can adopt a higher-level declarative abstraction (docker stack). Docker stack describes collections of services, volumes, networks, and other configuration abstractions.\nThese environments are described using Docker Compose V3 file format.\nCompose files use YAML language, such as docker-compose.yml.\nA YAML primer Comment support: ( #)\nThree types of data: maps (key: value, the value can be double-quote style or plain style), lists (start by -), scalar values\nTwo styles: flow collections, block style.\nUse indentation to indicate content scope. (only use spaces)\nCollections of services with Compose V3 docker stack deploy subcommands: create and update stacks.\nDocker compose cannot handle delete well: e.g., The Compose file you provided to the stack deploy command did not have any refer to the mariadb service, so Docker did not make any changes to that service.\nusing docker service remove executing docker stack deploy with the \u0026ndash;prune flag Stateful services and preserving data Define volumes in compose file:\nvolumes: volume_name: property (empty definition uses volume defaults)\nThe top-level property defines the volumes that can be used by services within the file.\nDocker can attach the new service to the original volume.\nLoad balancing, service discovery, and networks with Compose Port publishing for a service is different from publishing a port on a container: containers directly map the port on the host interface to an interface for a specific container, services might be made up of many replica containers.\nFor docker services, docker creating virtual IP (VIP) addresses and balancing requests for a specific service between all of the associated replicas.\nFirst network ingress: handles all port forwarding from the host interface to services. Created when you initialize Docker in swarm mode. Second network: shared between all of the services in your stack. Services default attach to a network named default. The address listed here is the container address, or the address that the internal load balancer would forward requests onto. Not the virtual IP addr you would see when inspecting. Define network in compose file:\ntop-level networks property that includes network definitions networks property of services\n"},{"id":1,"href":"/posts/first-post/","title":"First Post","section":"Posts","content":" HEllo world # "},{"id":2,"href":"/stage2/k8s-in-action/","title":"K8s in Action","section":"Stage2s","content":"第一章：Kubernetes 介绍\nKubernetes 通过对实际硬件做抽象，然后将自身暴露成一个平台，用于部署和运行应用程序。它允许开发者自己配置和部署应用程序，而不需要系统管理员的帮助，让开发者聚焦于保持底层基础设施运转正常的同时，不需要关注实际运行在平台上的应用程序。\nK8s系统由一个主节点和若干个工作节点组成。开发者把一个应用列表提交到主节点，k8s会把它们部署到集群的工作节点。组件被具体部署在哪个节点对于开发者和系统管理员来说都不用关心。\n第二章：开始使用K8S和Docker\nkubectl cluster-info 展示集群信息\nkubectl get 列出各种kubernetes对象的基本信息, e.g., kubectl get nodes kubectl describe node xxx 查看更详细的信息\n为kubectl创建别名 alias k=kubectl , tab命令补全\npod: 一组紧密相关的容器，运行在同一个工作节点上，同一个linux命名空间内\n不能用kubectl get 列出单个容器，因为它们不是独立的k8s对象，但是可以列出pod，kubectl get pods\n调度scheduling：将pod分配给一个节点\nkubectl expose re kubia \u0026ndash;type=LoadBalancer \u0026ndash;name kubia-http 创建外部的负载均衡，可以通过负载均衡的公共ip访问pod\nkubia-http服务：解决不断变化的pod IP地址的问题\nReplicationController：负责pod并让它们保持运行\n第三章：pod: 运行于Kubernetes中的容器 介绍pod\n一个pod不会跨越多个工作节点\n想象一个由多个进程组成的应用程序，由于不能将多个进程聚集在一个单独的容器中（很难确定每个进程分别记录了什么），我们需要用pod将容器绑定在一起，作为一个单元进行管理\nKubernetes 通过配置Docker 来让一个pod 内的所有容器共享相同的Linux 命名空间、网络、IPC（进程间通信）命名空间，可以通过IPC进行通信。也可以共享PID命名空间。\n同一pod 中的容器运行的多个进程需要注意不能绑定到相同的端口号（共享network命名空间），否则会导致端口冲突。\n容器可以通过localhost与同一pod中的其他容器进行通信。\n每个pod都可以通过其他pod 的IP 地址来实现相互访问，它们之间没有NAT (网络地址转换） 网关，不管是不是在不同的server上。\n这个平面网络是由额外的软件基于真实链路实现的。\n通过pod合理管理容器\n我们应该将应用程序组织到多个pod 中， 而每个pod 只包含紧密相关的组件或进程。\n多层应用分担到多个pod中，提高基础架构的利用率 pod是扩缩容的基本单位，不同组件有不同的扩缩容要求\n以YAML或JSON描述文件创建pod\nYAML包含：\nKubernetes API 版本 YAML描述的资源类型 metadata spec status （运行时的只读数据，创建pod时不需要提供）\n指定容器端口纯粹是展示性的（informational），但是仍然是有意义的，这样每个使用集群的人都可以快速查看每个pod对外暴露的端口。\nkubectl explain pods 来查看pods支持哪些属性，例如kubectl explain pod.spec\nkubectl create -f xxx.yaml 从YAML文件或JSON创建pod（及其他k8s资源）\nkubectl get po xxx -o yaml 查看该pod的完整描述文件\nkubectl logs xxx 获取pod日志\nkubectl logs pod_name -c container_name pod被删除后，日志也会被删除（需要设置集中的日志系统）\n通过向pod发送请求来测试pod：\n将本地网络端口转发到pod中的端口：kubect1 port-forward kubia-manual 8888:8080\n通过端口转发连接到pod：另开一个终端，curl localhost:8888\n使用标签组织pod\n标签是可以附加到任意k8s资源的键值对。\n一个资源可以拥有多个标签，在创建资源时可以将标签附加到资源上， 之后也可以再添加其他标签， 或者修改现有标签的值。\nkubectl get pods \u0026ndash;show-labels 列出所有标签\nkubectl get po -L xxx,xxx 列出指定的标签\nkubectl label po xxx creation_method=manual 添加标签\nkubectl label po xxx creation_method=manual \u0026ndash;overwrite 更改标签\n通过标签选择器列出pod子集\nkubectl get po -l xxx 标签选择器，多个条件的话用，隔开\n使用标签选择器来约束pod调度\n使用节点标签和节点标签选择器来描述pod对于节点的需求， 使Kubemetes选择一个符合这些需求的节点。\n运维团队向集群添加新节点时，通过标签对节点进行分类，e.g., gpu=true, 唯一标签key为kubernetes.io/hostname，值为该节点的实际主机名\n将pod调度到特定节点：在spec部分添加一个nodeSelector字段\n注解pod\n注解也是键值对，但是不能像标签一样用于对对象进行分组。\n用作为pod或其他对象添加说明，使得使用者可以快速了解信息\n查找对象的注解：获取完整YAML或者JSON文件（k get po xxx -o yaml 或 k describe pod xxx ）\nkubectl annotate pod kubia-manual mycompany.com/someannotation=\u0026ldquo;foo bar\u0026rdquo; 添加注解\n使用命名空间对资源进行分组\n想将对象分割成完全独立且不重叠的组，允许我们多次使用相同的资源名称（跨不同的命名空间）\n如果有多个用户或用户组正在使用同一个Kubernetes 集群，并且它们都各自管理自己独特的资源集合，那么它们就应该分别使用各自的命名空间。不用担心删除更改其他用户的资源，无需担心名称冲突\n命名空间为资源名称提供了一个作用域\n发现、创建、管理命名空间\nk get ns 列出集群中所有的命名空间\nk get po \u0026ndash;namespace(or: -n) xxx 列出xxx命名空间下的pod（默认是列出default下的）\n从YAML文件创建命名空间\nk create namespace xxx 创建命名空间\n命名空间的名字不允许包含点号\n在特定命名空间中创建资源\nYAML文件的metadata字段添加 namespace:xxxx k create -f xxx.yaml -n name_space\nkubectl config 管理默认的命名空间\nkubectl config set context $ (kubectl config current context) \u0026ndash; namespace xxx\n命名空间也行不提供网络隔离（看具体配置）等等隔离\n停止和移除pod\n30秒\n按名称删除pod\nkubectl delete po xxx1 xxx2 xxx3 使用标签选择器删除pod\nk delete po -l xxx=xxx 删除整个命名空间来删除pod\nk delete ns xxx 删除命名空间中的所有pod，但保留命名空间\nk delete po \u0026ndash;all 删除当前命名空间中的所有pod\n删除命名空间中的（几乎）所有资源\nk delete all \u0026ndash;all 第四章：副本机制和其他控制器：部署托管的pod 保持pod健康：存活探针 (liveness probe)\n有三种探针机制：\nHTTP GET探针 TCP套接字探针 Exec探针\n应该给探针设置initialDelaySeconds\n查看之前容器为什么终止，应该看前一个pod的日志，而不是当前pod的：k logs mypod \u0026ndash;previous\n通过k describe po xxx来了解po状态，关注其中的exit code和Events\nExit Code: 128+x, 其中x是终止进程的信号编号。e.g., 137=128+9，9是SIGKILL的信号编号\n如何创建有效的存活探针：\n应该给探针设置initialDelaySeconds，留出应用程序的启动时间 存活探针应该检查什么，应用程序内部 保持探针轻量 无需在探针中实现重试循环\n存活探针运行在pod工作节点上的kubelet，部署运行在主服务器上。如果工作节点本身崩溃，存活探针无法执行任何操作，这时候需要ReplicationController。\nReplicationController\nReplicationController是一种k8s资源，可以确保pod始终保持运行状态\nReplicationController的协调流程\nReplicationController的三个部分：\nlabel selector 标签选择器，用于确定RC作用域中有哪些pod 更改后对现有的pod没有影响，会使现有的pod脱离RC的范围，控制器会停止关注它们 replica count 副本个数，指定运行的pod数量 pod template pod模板，用于创建新的pod副本 更改后只影响RC新创建的pod 创建RC\npod模板中的标签要与RC的selector一致，或者省略selector标签，它会自动根据pod模板中的标签来配置。\n修改pod模板\n不影响旧的pod\nk edit rc kubia 修改pod模板\n水平缩放pod\n陈述式\n更改rc.yaml文件中的spec.replicas\n删除RC\nk delete rc 删除rc，由rc管理的pod也会被删除\nk delete rc \u0026ndash;cascade=false/orphan 删除rc，保留由rc创建的pod\n使用ReplicaSet而不是ReplicationController\n二者区别：ReplicaSet的标签选择器更强\n支持标签组，e.g., RC无法将pod与env=production和env-devil同时匹配，ReplicaSet可以 支持仅基于标签名的存在来匹配pod，类似于env=* 定义ReplicaSet\nk create -f kubia-replicaset.yaml 报错 no matches for kind \u0026ldquo;ReplicaSet\u0026rdquo; in version \u0026ldquo;apps/v1beta2\u0026rdquo;。\n解决方案：查看版本kubectl api-versions，改成app/v1\nk get rs： replicaset的缩写是rs\nk describe rs\nReplicaSet强大的标签选择器\n使用DaemonSet在每个节点上运行一个pod\n希望在集群中的每个节点上运行一个pod（例如日志收集器、资源监控器、kube-proxy进程等系统服务），需要创建DaemonSet对象\nDaemonSet的调度不受节点不可调度属性的影响\n通过pod模板中的nodeSelector属性指定部署到哪些特定的节点\nk get ds DaemonSet缩写是ds\nk label node node_name disk=ssd 给节点打标签\n更改node的标签后，由ds管理的pod也会被删除\n运行执行单个任务的pod\nJob资源：运行一个pod，这个pod在任务完成后，变成完成状态。例如，临时任务（需要以正确的方式结束）\n在发生节点故障时，该节点上由Job 管理的pod将按照ReplicaSet 的pod 的方式，重新安排到其他节点。\n如果进程本身异常退出（进程返回错误退出代码时）， 可以将Job 配置为重新启动容器。\n定义Job资源\n其中的restartPolicy属性指定容器中运行的进程结束时，k8s会做什么。默认是always，但是job资源不能使用默认策略。因此要将restartPolicy设定为OnFailure或者Never\n在Job中运行多个pod实例\n也可也通过缩放job来更改parallelism属性：k scale job job_name \u0026ndash;replicas 3\n限制Job pod完成任务的时间\n在pod template中设置activeDeadlineSeconds属性来限制pod时间。如果超过这个时间，系统就会终止pod，并将job标记为失败。\n安排Job定期运行或在将来运行一次 创建CronJob\ncron时间表格式，从左到右分别是 分钟 小时 每月中的第几天 月 星期几。\n对于对时间要求较高的任务，可以设置startingDeadlineSeconds来设置截止时间。到了截止时间不执行任务会直接failed。\n服务：让客户端发现pod并与之通信 介绍服务\nk8s服务：为一组相同功能的pod提供单一不变的接入点的资源。\n服务存在时，它的IP地址和端口号不会改变（pod的IP地址可能会改变），客户端通过IP和端口号建立连接，连接会被路由到提供该服务的任意一个pod上。因此，客户端不需要知道每个pod的地址。\n服务通过标签选择器来指定pod\n创建服务 k expose 通过yaml文件\n在80端口接受请求，然后路由到标签选择器app=kubia的pod的8080端口上\nservice的缩写是svc\n在运行的容器中远程执行命令\nk exec pod_name \u0026ndash; curl -s http://10.111.249.153\n其中，双横杠“\u0026ndash;”表示k命令项的结束，双横杠之后的内容是pod内部需要执行的命令\n会话亲和性\n希望特定客户端产生的所有请求每次都指向同一个pod\nspec.sessionAffinity = ClientIP 会话亲和性不能基于cookie\n端口命名\n同一个服务可以暴露多个端口（需要给每个端口指定名字），但是标签选择器是针对整个服务的，不能对每个端口做单独的配置\n可以命名端口，好处是即使更改端口号也无需更改服务spec\n服务发现\n客户端pod如何知道服务的IP和端口？\n通过环境变量发现服务\n如果创建的服务早于客户端pod的创建，可以客户端pod可以通过环境变量获取服务的IP和端口号\nKUBIA_SERVICE_HOST 服务集群的IP 和 KUBIA_SERVICE_PORT 服务所在的端口，所有字母大写，横杠被转化成下划线\n通过DNS发现服务\npod kube-dns 运行DNS服务，集群中的其他pod使用其作为dns（k8s通过修改每个容器的/etc/resolv.conf实现）\npod是否使用内部DNS服务器是根据pod中spec的dnsPolicy属性来决定的\n通过全限定域名（Fully qualified domain name,FQDN）连接服务\nFQDN链接：服务名称.命名空间.svc.cluster.local\n其中svc.cluster.local是集群本地服务名称中使用的可配置集群域后缀\n如果两个pod在同一个命名空间下，可以省略命名空间和svc.cluster.local后缀\n无法ping通服务IP的原因\n在pod内ping服务ip无法成功，因为这是一个虚拟IP，只有在和服务端口结合时才有意义\n连接集群外部的服务\n通过k8s service的特性暴露外部服务，不要让服务将连接重定向到集群中的pod，而是重定向到外部IP和端口\n可以利用服务负载均衡的特点\n介绍Endpoints资源\nEndpoints资源：暴露一个服务的IP地址和端口的列表\nk get endpoints kubia 缩写ep\n尽管定义了Selector标签选择器，但是在重定向传入连接时不会直接使用它。标签选择器用于构建IP和端口列表，然后储存在Endpoint资源中。在客户端连接到服务时，服务代理选择这些IP和端口对中的一个，并将传入连接重定向到该位置监听的服务器\n手动配置服务的endpoint\n创建没有标签选择器的服务\n为没有标签选择器的服务创建Endpoint资源\nEndpoints对象需要与服务具有相同的名称\n其中，11.11.11.11:80和22.22.22.22:80是外部IP:端口的例子\n为外部服务创建别名\n通过FQDN访问外部服务\n服务创建后，可以通过external-service.default.svc.cluster.local域名（甚至external-service）连接到外部服务，不需要外部服务实际的FQDN\nExternalName服务仅在DNS级别实施，因此连接到服务的客户端将直接连接到外部服务，绕过了服务代理\n将服务暴露给外部客户端\n需要向外部公开某些服务，例如前端web服务器，以便外部客户端可以访问它们\n三种方式：\n将服务的类型设置成NodePort 将服务的类型设置成LoadBalance 创建一个Ingress资源 使用NodePort类型的服务\n可以通过内部集群IP访问NodePort服务，还可以通过任何节点IP和预留节点端口访问NodePort服务\n指定集群节点的节点端口不是必需的。如果忽略，k8s会随机选择一个端口\n可以通过以下地址访问该服务：\n10.100.138.138:80 \u0026lt;1st node\u0026rsquo;s IP\u0026gt;:30123 \u0026lt;2nd node\u0026rsquo;s IP\u0026gt;:30123\n到达任何一个节点30123端口的传入连接将被重定向到一个随机选择的pod，该pod是否位于接收到连接的节点上是不确定的\n创建LoadBalance类型的服务\nLoadBalancer类型的服务是一个具有额外的基础设施提供的负载均衡器NodePort服务\n了解外部连接的特性 防止不必要的网络跳数\n当外部客户端通过节点端口连接到服务时，随机选择的pod不一定在接受连接的同一节点上运行，需要额外的网络跳转才能到达pod\n可以将服务配置为仅将外部通信重定向到接收连接的节点上运行的pod来阻止此额外跳数（local外部流量策略）\n如果没有本地pod存在，连接将挂起，并不会转发到随机的全局pod（缺点一，需要负载均衡器将连接转发给至少具有一个pod的节点）\n缺点二：可能导致负载分配不均衡\n客户端IP是不记录的\n当通过节点端口接收到客户端的连接时， 由于对数据包执行了源网络地址转换(SNAT), 因此数据包的源IP将发生更改，无法得知实际的客户端IP\n如果使用local外部流量策略，那么IP不会更改，因为在接受连接的节点和托管目标pod的节点之间没有额外的跳跃。\n通过Ingress暴露服务\nIngress只需要一个公网IP就能为许多服务提供访问，在网络栈（HTTP）应用层操作\nNo ingress pod on the server\nIngress工作原理：不会直接将请求转发给该服务，而是用它来选择一个pod\n通过相同的ingress暴露多个服务 将不同的服务映射到相同主机的不同路径\npath和rules都是数组。可以根据URL中的不同路径，通过同一个IP地址（Ingress控制器的IP地址）访问两个不同的服务\n将不同的服务映射到不同的主机上\nDNS需要将foo.example.com和bar.example.com都指向ingress控制器的IP地址\n配置Ingress处理TLS传输\n客户端和控制器之间的通信是加密的，而控制器和后端pod之间的通信则不是\nIngress负责处理与TLS相关的所有内容\npod就绪后发出信号 介绍就绪探针 就绪探针类型：Exec探针、HTTP GET探针、TCP socket探针\n向pod中添加就绪探针\n使用headless服务来发现独立的pod 将service的spec中的clusterIP设置为None，就会使服务成为headless服务。\nk8s不会为headless服务分配集群IP，客户端可以通过这个IP连接到支持它的pod\n"}]